---
title: "CPO2"
author: "Jose López Galdón"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---


```{r setup, include = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


[//]: Librerías

```{r Libraries, include = FALSE}

library(here) # Comentarios [//]:
library(tidyverse)
library(janitor) # Limpieza de nombres
library(skimr) # Summary pro
library(magrittr) # Pipe operators %<>%
library(corrplot) # Gráfico de correlaciones
library(ggcorrplot)  # Correlaciones con ggplot
library(PerformanceAnalytics) # Otra correlación
library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches

```

## CARGAMOS LOS DATOS

```{r load data, include = FALSE}

raw_data <- read_csv("data/01_raw/nba.csv")

```

Visualizamos los datos.

```{r view data}

raw_data

```

***
***

## LIMPIEZA DEL DATASET

### Renombrar columnas

El problema que tenemos, es que los nombres de las columnas no tienen el formato adecuado, esto es porque presentan símbolos como %.

```{r clean names}

# Para ello usaremos la funcion clean_names() que nos cambiará aquellos valores que nos puedan ocasionar problemas a la hora de trabajar con las variables

raw_data %<>% clean_names()   # %<>%: Evita poner "raw_data <- raw_data %>%"
colnames(raw_data)

```

Una vez tenemos los nombres de las variables en el formato correcto, ya podemos comenzar con el tratamiento de *duplicados* y de *valores nulos*

***

### Data wrangling

La manipulación de datos es el proceso de limpieza y unificación de conjuntos de datos complejos para el análisis, lo que a su vez aumenta la productividad dentro de una organización.

```{r data wrangling}

# Valores duplicados: los eliminamos

raw_data %<>% distinct(player, .keep_all = T)

# Valores nulos:

summarise_all(raw_data, funs(sum(is.na(.))))
  
  # Como existen pocos valores nulos los eliminaremos

raw_data %<>% drop_na()

```

***
***

## EDA

A continuación, realizaremos un resumen estadístico de las variables, diagramas de dispersión así como un breve estudio de las correlaciones.

### Estadísticos relevantes 

```{r skim}

# Con el comando skim() de la librería skimr podemos hacer un summary más completo

skim(raw_data)

```

Como podemos observar más arriba, sería interesante estudiar aquellas variables cuya variabilidad (desviación respecto a la media) sea nula o cercana a cero, ya que esto nos indica que estas variables no son muy importantes ya que apenas varían.

***

### Diagramas de dispersión

```{r scatterplots, fig.height = 20, fig.width = 8, fig.align = "center"}

raw_data %>% 
  select_at(vars(-c("player", "nba_country", "tm"))) %>%          # Seleccionamos todas las variables menos las categóricas
    tidyr::gather("id", "value", 2:25) %>%
      ggplot(., aes(y = salary, x = value)) +                     # Hacemos un ggplot con la variable salario en el eje y
        geom_point() +
        geom_smooth(method = "lm", se = FALSE, color = "forestgreen") +
        facet_wrap( ~ id, ncol = 4, scales = "free_x")            # Utilizamos facet_wrap para ver todas los gráficos

```

Como podemos observar, parece que presentamos problemas con la variable _salary_, para ello aplicaremos el logaritmo a la variable para ver si mejora.

```{r log(salary) scatterplots, fig.height = 20, fig.width = 8, fig.align = "center"}

# Aplicamos el mismo código que antes, pero con el log(salary)

raw_data %>% 
  select_at(vars(-c("player", "nba_country", "tm"))) %>%            
    tidyr::gather("id", "value", 2:25) %>%
      ggplot(., aes(y = log(salary), x = value)) +                    
        geom_point() +
        geom_smooth(method = "lm", se = FALSE, color = "forestgreen") +
        facet_wrap( ~ id, ncol = 4, scales = "free_x")            

```

Como podemos observar ha mejorado un poco, por lo que mantendremos el cambio a `log(salary)`.

```{r log(salay)}

log_data <- raw_data %>% mutate(salary = log(salary))

```


### Correlaciones

#### Gráfico de correlaciones estándar

```{r correlations, fig.height = 8, fig.width = 8, fig.align = "center", warning = FALSE}

# Generamos un vector con la informacion de aquellas variables que no analizamos

cat_vars <- c("player", "nba_country", "tm")

# Gráfico de correlaciones

corrplot(cor(log_data %>% 
               select_at(vars(-cat_vars)), 
             use = "complete.obs"), 
         method = "circle", type = "upper")

```

Como podemos observar, las variables _g_ (partidos jugados), _mp_ (minutos jugados), _ows_, _dws_ y _ws_ están ciertamente correlaciandas con el salario, sin embargo, tenemos correlaciones entre estas propias variables, en concreto, _g_ y _mp_ están correlacionadas, esto nos dará problemas posteriormente.

***

#### Heatmap

```{r heatmap, fig.height = 8, fig.width = 10, fig.align = "center"}

# Recordemos que eliminamos las variables que no queremos (cat_vars)

ggcorrplot(cor(log_data %>% 
               select_at(vars(-cat_vars)), 
            use = "complete.obs"),
            hc.order = TRUE,
            type = "lower",  lab = TRUE, digits = 1, colors = c("red", "white", "steelblue"))

```

En este gráfico tenemos un _heatmap_ o mapa de calor con las correlaciones entre las variables. De esta manera, podemos observar como los tiros triples apenas influyen en el salario.

***

#### Chart correlation

```{r chart.correlation, fig.height = 20, fig.width = 20, fig.align = "center"}

chart.Correlation(log_data %>% 
               select_at(vars(-cat_vars)),
               histogram = TRUE, pch = 19)

```

El gráfico superior es muy interesante porque nos muestra los histogramas con las densidades en la diagonal, los gráficos de dispersión a la izquierda y las correlaciones a la derecha.

***
***

## MÉTODOS DE CONCETRACIÓN (SHRINKAGE METHODS)

La reducción de las estimaciones de los coeficientes tiene el efecto de reducir significativamente su varianza. Las dos técnicas más conocidas para reducir las estimaciones de coeficientes hacia cero son la regresión de cresta (Ridge) y el lazo (Lasso).

***

### Ridge

La regresión Ridge es similar a los mínimos cuadrados, excepto que los coeficientes se estiman minimizando una cantidad ligeramente diferente.

La regresión de Ridge, como OLS, busca estimaciones de coeficientes que reducen el RSS, sin embargo, también tienen una penalización por contracción cuando los coeficientes se acercan a cero.

Esta penalización tiene el efecto de reducir las estimaciones del coeficiente hacia cero.

Un parámetro, λ, controla el impacto de la contracción. λ = 0 se comportará exactamente como la regresión OLS. Por supuesto, la selección de un buen valor para λ es crítica, y debe elegirse utilizando técnicas de validación cruzada.


```{r seed & training set}

# Generamos una semilla con la fecha en la que realizamos el modelo para pode reproducirlo en más ocasiones

set.seed(29102020)

# Vamos a utilizar una proporción 80% para el training y 20% para el test

nba_split <- initial_split(log_data, prop = .8, strata = "salary")

# Definimos nuestras bases de training y test

nba_train <- training(nba_split)
nba_test  <- testing(nba_split)

```


```{r discard intercept test & train}

# A continuación, eliminaremos el intercepto

nba_train_x <- model.matrix(salary ~ . -player -nba_country -tm, data = log_data)[, -1]
nba_train_y <- log_data$salary

nba_test_x <- model.matrix(salary ~ . -player -nba_country -tm, data = log_data)[, -1]
nba_test_y <- log_data$salary

# ¿Cuál es la dimensión de nuestro dataset? Esto lo utilizamos para comprobar que está ok
dim(nba_train_x)

```
Para realizar un regresión cresta podemos usar la función `glmnet :: glmnet`. El parámetro `alpha` le dice a `glmnet` que realice una regersión cresta (`alpha = 0`), lasso (`alpha = 1`) o elastic net ($ 0  leq alpha  leq 1 $). `glmnet` está haciendo dos cosas que debes tener en cuenta:

1. Es fundamental que las variables independientes (x’s) estén estandarizadas al realizar una regresión regularizada. `glmnet` realiza esto por nosotros. Si estandariza sus predictores antes de glmnet, se puede desactivar este argumento con `standardize = FALSE`.

2. `glmnet` realizará regresiones crestas para una amplia gama de parámetros $  lambda $, que se ilustran en la siguiente figura.

```{r ridge regression, fig.height = 8, fig.width = 10, fig.align = "center"}

# Aplicamos la regresión ridge, recordemos que alpha = 0

nba_ridge <- glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 0
)

# Hacemos un plot

plot(nba_ridge, xvar = "lambda")

```

En el gráfico situado más arriba tenemos representadas los coeficientes de las variables en función del lamnda que escojamos, a mayor lambda los coeficientes de las variables tenderán a cero,

#### Ajuste de λ

```{r tuning lambda (ridge), fig.align = TRUE}

# Aplicamos cross-validation

nba_ridge_cv <- cv.glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 0
)

# Representamos gráficamente

plot(nba_ridge_cv)

```

En el gráfico superior tenemos representados los MSE (Error Cuadrático Medio) en los puntos rojos con sus respectivas desviaciones. A su vez, tenemos dos líneas verticales punteadas, la primera de ellas (situada cerca del -2) indica el menor MSE y la que se encuentra a la derecha del 0 es la que tiene menor desviación y menor error.

```{r minimum MSE}

# Calculamos el mínimo MSE

min(nba_ridge_cv$cvm)

# Lambda del mínimo

nba_ridge_cv$lambda.min

# Posición en la gráfica

log(nba_ridge_cv$lambda.min)

```

Tras el cálculo, obtenemos que el mínimo MSE es 1.13911, el λ es 0.166034 y se encuentra en -1.795563.

```{r 1 st. error of min MSE}

# Calculamos el mínimo MSE para la menor desviación

nba_ridge_cv$cvm[nba_ridge_cv$lambda == nba_ridge_cv$lambda.1se]

# Lambda del mínimo para la menor desviación

nba_ridge_cv$lambda.1se

# Posición en la gráfica

log(nba_ridge_cv$lambda.1se)

```

En esta ocasión, el MSE es de 1.218333, el λ es 1.171337 y se encuentra en 0.158146.

```{r ridge regression with lambda, fig.height = 8, fig.width = 10, fig.align = "center"}

# Dibujamos el mismo plot incluyendo la ubicación del lambda

plot(nba_ridge, xvar = "lambda")
abline(v = log(nba_ridge_cv$lambda.1se), col = "red", lty = "dashed")

```

La línea roja punteada representa el mejor lambda con ridge (reocrdemos que es el que menor desviación tiene con el mernor MSE), de esta manera muchas de las variables son cercanas a cero, sin embargo tendremos 3 variables con coeficientes negativos y una con positivos, esto lo podemos ver mejor en el siguiente gráfico:

```{r influencial plot, fig.align = TRUE, warning = FALSE}

coef(nba_ridge_cv, s = "lambda.1se") %>%
  broom::tidy() %>%
    filter(row != "(Intercept)") %>%                # Eliminamos el intercepto
      ggplot(aes(value, reorder(row, value))) +     
        geom_point(color = "forestgreen") +
        ggtitle("Influencia de las 24 variables en el modelo") +
        xlab("Coefficient") +
        ylab(NULL)

```

En este gráfico podemos ver claramente lo que comentabamos anteriormente, como _ts_percent_ es la variable con coeficiente positivo, mientras que _x3p_ar_, _f_tr_ y _ws_48_ son las que tienen coeficientes negativos.

***

### Lasso

La regresión Ridge tenía al menos una desventaja; incluye todos los predictores p en el modelo final. El término de penalización establecerá muchos de ellos cerca de cero, pero nunca exactamente a cero.

En general, esto no es un problema para la precisión de la predicción, pero puede hacer que el modelo sea más difícil de interpretar los resultados.

Lasso supera esta desventaja y es capaz de forzar algunos de los coeficientes a cero, dado que s es lo suficientemente pequeño. Como s = 1 da como resultado una regresión OLS regular, cuando s se acerca a 0, los coeficientes se reducen a cero. Por lo tanto, la regresión de Lasso también realiza la selección de variables.

```{r lasso regression,  fig.height = 8, fig.width = 10, fig.align = "center"}

# Aplicamos la regresión lasso, recordemos que alpha = 1

nba_lasso <- glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 1
)

# Hacemos un plot

plot(nba_lasso, xvar = "lambda")

```

Podemos observar como ha cambiado el gráfico respecto al realizado en la regresión Ridge. Continuaremos calculando el λ ideal.

#### Ajuste de λ

```{r tuning lambda (lasso), fig.align = TRUE}

# Aplicamos cross-validation

nba_lasso_cv <- cv.glmnet(
  x = nba_train_x,
  y = nba_train_y,
  alpha = 1
)

# Representamos gráficamente

plot(nba_lasso_cv)

```

En el gráfico superior tenemos representados los MSE (Error Cuadrático Medio) en los puntos rojos con sus respectivas desviaciones. En la regresión Lasso tenemos dos posibles λ, el que sitúa más a la izquierda en torno al -3 tiene ser variables que explican el modelo, mientras que la línea punteada de la derecha son 5 variables las que explican este modelo, en principio nos quedaríamos con la que menos variables tenga para explicar el modelo, ya que es más sencilla.

```{r minimum MSE (lasso)}

# Calculamos el mínimo MSE

min(nba_lasso_cv$cvm)

# Lambda del mínimo

nba_lasso_cv$lambda.min

# Posición en la gráfica

log(nba_lasso_cv$lambda.min)

```

Tras el cálculo, obtenemos que el mínimo MSE es 1.096106, el λ es 0.05829773 y se encuentra en -2.842192.

```{r 1 st. error of min MSE (lasso)}

# Calculamos el mínimo MSE para la menor desviación

nba_lasso_cv$cvm[nba_lasso_cv$lambda == nba_lasso_cv$lambda.1se]

# Lambda del mínimo para la menor desviación

nba_lasso_cv$lambda.1se

# Posición en la gráfica

log(nba_lasso_cv$lambda.1se)

```

En esta ocasión, el MSE es de 1.170946, el λ es 0.1780328 y se encuentra en -1.725787.

```{r lasso regression with lambda, fig.height = 8, fig.width = 10, fig.align = "center"}

# Dibujamos el mismo plot incluyendo la ubicación del lambda

plot(nba_lasso, xvar = "lambda")
abline(v = log(nba_lasso_cv$lambda.1se), col = "red", lty = "dashed")

```

En nuestro caso, hemos escogido el modelo nás sencillo, el que presenta 5 variables, como podemos observar, con este λ casi todos los coeficientes son cercanos a 0.

```{r influencial plot (lasso), fig.align = TRUE, warning = FALSE}

coef(nba_lasso_cv, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influencia de las 5 variables de la regresión Lasso") +
  xlab("Coefficient") +
  ylab(NULL)

```

Hemos representado en azul aquellas variables con coeficiente positivo, estas son _age_, _ws_, _drb_percent_ y _mp_, mientras que la única negativa es _nba_draft_number_.


De esta manera, podemos concluir que el modelo Lasso es mejor por dos motivos, presenta un menor MSE y es más sencilla, ya que contine tan solo 5 variables, mientras que el Ridge incluye 24.

***

### Elastic Net

La red elástica es un método de regresión regularizado que combina lienalmente las penalizaciones de los métodos de Lasso y Ridge.

```{r elastic net regression, fig.height = 16, fig.width = 8, fig.align = "center"}

# Definios nuestros 3 modelos, vamos a crear un elastic net con un alpha de 0.5

lasso    <- glmnet(nba_train_x, nba_train_y, alpha = 1.0)

elastic <- glmnet(nba_train_x, nba_train_y, alpha = 0.5)

ridge    <- glmnet(nba_train_x, nba_train_y, alpha = 0.0)

# Hacemos un plot para visualizarlos

par(mfrow = c(3, 1), mar = c(6, 4, 6, 2) + 0.1)

plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)")
plot(elastic, xvar = "lambda", main = "Elastic Net (Alpha = 0.5)")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)")
```

Una vez tenemos representados los 3 modelos, pasaremos a escoger el mejor de ellos, modificando los parámetros λ  y α.

#### Ajuste de λ y α

```{r tunning lambda and alpha (elastic net)}

# Mantemeos los mismos folds

fold_id <- sample(1:10, size = length(nba_train_y), replace = TRUE)

# Vamos a crear una tibble que contenga los alphas desde 0 hasta 1, de 0.01 en 0.01, de manera que podemos escoger los dos mejores parámetros (aplha y lambda)

tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
tuning_grid

# Hacemos un loop que nos complete la tabla

for(i in seq_along(tuning_grid$alpha)) {
  
  # Añademe el modelo de cross validation correspondiente para dicho alpha
  
  fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
  # Obtenemos los valores de alpha y lambda correspondientes
  
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

# Volvemos a cargar el tibble con la información

tuning_grid

```

A priori, podemos deducir que el modelo con alpha = 1.0, es decir, el modelo Lasso es el mejor, sin embargo vamos a dibujar un gráfico para comprobar si esto es cierto

```{r graph MSE, fig.align = 'center'}

# Dibujamos los modelos anteriores con la desviación estándar para ver si es cierto que el modelo Lasso es el mejor

tuning_grid %>%
  mutate(se = mse_1se - mse_min) %>%
    ggplot(aes(alpha, mse_min)) +
      geom_line(size = 2) +
      geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
      ggtitle("MSE ± one standard error")

```

Como están todos dentreo de nuestro intervalo de confianza, todos son válidos. Por lo tanto, escogemos el más simple, en este caso el LASSO.

***
***

## PREDICCIÓN

Por último, realizaremos la predicción del modelo lasso.

```{r prediction lasso}

# Calculamos el mínimo MSE para Lasso

cv_lasso   <- cv.glmnet(nba_train_x, nba_train_y, alpha = 1.0)
min(cv_lasso$cvm)

# Realizamos el cálculo con el dataset de training

pred <- predict(cv_lasso, s = cv_lasso$lambda.min, nba_test_x)
mean((nba_test_y - pred)^2)

```







